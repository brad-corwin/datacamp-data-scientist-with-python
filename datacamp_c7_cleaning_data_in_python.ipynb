{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing dataset for analysis.\n",
    "\n",
    "Outliers are considerably higher or lower than the rest of the data. Investigate further for data cleaning.\n",
    "\n",
    "Not all outliers are bad data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape() # to see the structure\n",
    "df.info() # to get more information about a database\n",
    "df.describe() # shows more information about the database\n",
    "\n",
    "df.continent.value_counts(dropna=False) # perform a frequency count\n",
    "df.continent.value_counts(dropna=False).haed() # returns top categories\n",
    "# for example each country should have 1 observation but sweden has 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization helps point out outliers.\n",
    "\n",
    "Bar plots for discrete data counts\n",
    "\n",
    "Histograms for continous data\n",
    "\n",
    "Box plots visualize basic summary statistics\n",
    "\n",
    "Boxes show on the plot, the lines that extend from it are called whiskers. Outliers are values that are beyond the whiskers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.population > 1000000000] # find countries with over 1 billion people\n",
    "\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n",
    "\n",
    "# Create the boxplot\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)\n",
    "\n",
    "# Create and display the first scatter plot\n",
    "df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the 2 extreme outliers are in the borough of Manhattan. An initial guess could be that since land in Manhattan is extremely expensive, these outliers may be valid data points. Again, further investigation is needed to determine whether or not you can drop or keep those points in your data.\n",
    "\n",
    "When you want to visualize two numeric columns, scatter plots are ideal.\n",
    "\n",
    "In general, from the second plot it seems like there is a strong correlation between 'initial_cost' and 'total_est_fee'. In addition, take note of the large number of points that have an 'initial_cost' of 0. It is difficult to infer any trends from the first plot because it is dominated by the outliers.\n",
    "\n",
    "Tidy data provides a standard way to organize data values within a dataset - \"Tidy Data\" by Hadley Wickham\n",
    "\n",
    "3 principles of tidy data\n",
    "\n",
    "* columns represent separate variables\n",
    "\n",
    "* rows represent individual observations\n",
    "\n",
    "* observational units form tables\n",
    "\n",
    "some data is better for reporting and some is better for analysis. Tidy data makes it easier to fix common data problems.\n",
    "\n",
    "The problem is columns containing data values instead of variables.\n",
    "\n",
    "Fix this by using pd.melt()\n",
    "\n",
    "id_vars is a fixed column\n",
    "\n",
    "value_vars is which columns we want to melt\n",
    "\n",
    "var_name, value_name parameters to name the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day']) # don't want to melt id_vars\n",
    "# rename columns\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot is the opposite of melting\n",
    "\n",
    "Useful to turn analysis friendly to report friendly or when rows contain observations.\n",
    "\n",
    "Pivots can't work with duplicate entries. Can use Pivot Table instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index() # get back to original df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melting and pivoting are the basic tools. Another problem is when columns contain multiple bits of information.\n",
    "\n",
    "For example, want a separate sex and age group column instead of a combined column.\n",
    "\n",
    "1st melt data down\n",
    "\n",
    "2nd parse out sex and age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data may not always come in 1 huge file.\n",
    "\n",
    "Need to combine all the data.\n",
    "\n",
    "Concatenate two dataframes into a single dataframe. pd.concat(), don't forget to reset the index label ignore_index=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "axis=0 # row-wise concatenation\n",
    "axis=1 # col-wise concatenation\n",
    "ignore_index=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to concatenate dfs they must be in a list.\n",
    "\n",
    "What happens if there are thousands of files? glob function\n",
    "\n",
    "globbing - pattern matching for file names\n",
    "\n",
    "*.csv matches any csv\n",
    "\n",
    "file_?.csv matches any single character\n",
    "\n",
    "this returns the list of file names\n",
    "\n",
    "add the dataframes into a list to concatenate multiple files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine data (ex. organization of the data is not the same)\n",
    "\n",
    "can merge data instead, like based on column names\n",
    "\n",
    "* one-to-one \n",
    "\n",
    "* one-to-many where columns are populated with other tables data\n",
    "\n",
    "* many-to-many\n",
    "\n",
    "all use the same function\n",
    "\n",
    "Merge the site and visited DataFrames on the 'name' column of site and 'site' column of visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.dtypes\n",
    "\n",
    "object dtype is typically a string\n",
    "\n",
    "numeric cols can be strings or vice versa\n",
    "\n",
    "converting data to category\n",
    "\n",
    "* can make df smaller in memory\n",
    "\n",
    "* can make them utilized by other Python libraries for analysis\n",
    "\n",
    "if data should be float but come out as string, then bad data\n",
    "\n",
    "errors='coerce' means to convert values into a numeric value, invalid values will be set as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "\n",
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "\n",
    "# Print the info of tips\n",
    "print(tips.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "much of data cleaning involves string manipulation because the world's data is unstructured text\n",
    "\n",
    "also need to do string manipulation to make them consistent ex. 17, $17, $17.89\n",
    "\n",
    "re library for regular expressions for string pattern matching\n",
    "\n",
    "$ means to match from the end of the string so need to \\$\\d*\n",
    "\n",
    "/d*\n",
    "\n",
    "$17.00 /$/d*/./d*\n",
    "\n",
    "$17.89 /$/d*/./d{2}\n",
    "\n",
    "$17.895 ^\\$\\d*\\.\\d{2}$ where ^ matches the beginning and $ matches end, exact regex matching\n",
    "\n",
    "compile the pattern then use the pattern to match values like matching a value down pd columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regular expression module\n",
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('1123-456-7890')\n",
    "print(bool(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\d is the pattern required to find digits. This should be followed with a + so that the previous element is matched one or more times. This ensures that 10 is viewed as one number and not as 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')\n",
    "\n",
    "# Print the matches\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='\\w*', string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "complex cleaning sometimes requires multiple steps, can use a python function with .apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    \n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    \n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips.sex.apply(recode_sex)\n",
    "\n",
    "# Print the first five rows of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])\n",
    "\n",
    "# Print the head of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duplicate data skews results\n",
    "\n",
    ".drop_duplicates() to address\n",
    "\n",
    "missing data\n",
    "\n",
    "* leave as-is\n",
    "\n",
    "* drop them\n",
    "\n",
    "* fill missing value\n",
    "\n",
    "tips_nan.info() to get a count of missing value\n",
    "\n",
    "tips_nan.dropna() drops nans but you can lose too much data with this method\n",
    "\n",
    "you can also drop columns\n",
    "\n",
    ".fillna() - can fill with provided value ex. missing or 0, can fill with test statistic like mean, make sure the value makes sense, median is a better statistic to find the presence of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year', 'artist', 'track', 'time']]\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()\n",
    "\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)\n",
    "\n",
    "# Print the info of airquality\n",
    "print(airquality.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert statements check programmatically vs visually\n",
    "\n",
    "if we drop or fill NaNs, expect 0 missing values\n",
    "\n",
    "can write an assert statement to verify\n",
    "\n",
    "detect early warnings and errors\n",
    "\n",
    "true assert statements return nothing, false assert statements return an error\n",
    "\n",
    "The first .all() method will return a True or False for each column, while the second .all() method will return a single True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that there are no missing values\n",
    "assert pd.notnull(ebola).all().all()\n",
    "\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the scatter plot\n",
    "g1800s.plot(kind='scatter', x='1800', y='1899')\n",
    "\n",
    "# Specify axis labels\n",
    "plt.xlabel('Life Expectancy by Country in 1800')\n",
    "plt.ylabel('Life Expectancy by Country in 1899')\n",
    "\n",
    "# Specify axis limits\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_or_valid(row_data):\n",
    "    \"\"\"Function that takes a row of data,\n",
    "    drops all missing values,\n",
    "    and checks if all remaining values are greater than or equal to 0\n",
    "    \"\"\"\n",
    "    no_na = row_data.dropna()[1:-1]\n",
    "    numeric = pd.to_numeric(no_na)\n",
    "    ge0 = numeric >= 0\n",
    "    return ge0\n",
    "\n",
    "# Check whether the first column is 'Life expectancy'\n",
    "assert g1800s.columns[0] == 'Life expectancy'\n",
    "\n",
    "# Check whether the values in the row are valid\n",
    "assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "\n",
    "# Check that there is only one instance of each country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt gapminder: gapminder_melt\n",
    "gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')\n",
    "\n",
    "# Rename the columns\n",
    "gapminder_melt.columns = ['country', 'year', 'life_expectancy']\n",
    "\n",
    "# Print the head of gapminder_melt\n",
    "print(gapminder_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder.year)\n",
    "\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the series of countries: countries\n",
    "countries = gapminder['country']\n",
    "\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "\n",
    "# Write the regular expression: pattern\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "\n",
    "# Create the Boolean vector: mask\n",
    "mask = countries.str.contains(pattern)\n",
    "\n",
    "# Invert the mask: mask_inverse\n",
    "# Invert the mask by placing a ~ before it.\n",
    "mask_inverse = ~mask\n",
    "\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries.loc[mask_inverse]\n",
    "\n",
    "# Print invalid_countries\n",
    "print(invalid_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that country does not contain any missing values\n",
    "assert pd.notnull(gapminder.country).all()\n",
    "\n",
    "# Assert that year does not contain any missing values\n",
    "assert pd.notnull(gapminder.year).all()\n",
    "\n",
    "# Drop the missing values\n",
    "gapminder = gapminder.dropna(axis=0, how='any')\n",
    "\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1) \n",
    "\n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "\n",
    "# Use the .groupby() method on gapminder with 'year' as the argument. \n",
    "# Then select 'life_expectancy' and chain the .mean() method to it.\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "\n",
    "# Print the head of gapminder_agg\n",
    "print(gapminder_agg.head())\n",
    "\n",
    "# Print the tail of gapminder_agg\n",
    "print(gapminder_agg.tail())\n",
    "\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save both DataFrames to csv files\n",
    "gapminder.to_csv('gapminder.csv')\n",
    "gapminder_agg.to_csv('gapminder_agg.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
