{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".loc accesses using labels\n",
    "\n",
    ".iloc accesses using indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the row position of election.loc['Bedford']: x\n",
    "x = 4\n",
    "\n",
    "# Assign the column position of election['winner']: y\n",
    "y = 4\n",
    "\n",
    "# Print the boolean equivalence\n",
    "print(election.iloc[x, y] == election.loc['Bedford', 'winner'])\n",
    "\n",
    "# Create a separate dataframe with the columns ['winner', 'total', 'voters']: results\n",
    "results = election[['winner', 'total', 'voters']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df['eggs'] picks a column by default and returns a pandas series\n",
    "\n",
    "df.loc['Jan':'Apr',:] picks some rows and all columns\n",
    "\n",
    "can use lists in place of splices\n",
    "\n",
    "df['eggs'] returns a series by column name, 1D of labeled data\n",
    "\n",
    "df[['eggs']] returns a dataframe with a single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the row labels 'Perry' to 'Potter': p_counties\n",
    "p_counties = election.loc['Perry':'Potter',:]\n",
    "\n",
    "# Print the p_counties DataFrame\n",
    "print(p_counties)\n",
    "\n",
    "# Slice the row labels 'Potter' to 'Perry' in reverse order: p_counties_rev\n",
    "p_counties_rev = election.loc['Potter':'Perry':-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the columns from the starting column to 'Obama': left_columns\n",
    "left_columns = election.loc[:,:'Obama']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know exactly which rows and columns are of interest to you, this is a useful approach for subselecting DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new DataFrame: three_counties\n",
    "three_counties = election.loc[rows,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering is a more general tool for selecting part of the data based on properties of interest.\n",
    "\n",
    "When used to make a selection, a boolean series is called a filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boolean array: high_turnout\n",
    "high_turnout = election['turnout'] > 70\n",
    "\n",
    "# Filter the election DataFrame with the high_turnout array: high_turnout_df\n",
    "high_turnout_df = election[election['turnout'] > 70]\n",
    "\n",
    "# Create the boolean array: too_close\n",
    "too_close = election['margin'] < 1\n",
    "\n",
    "# Assign np.nan to the 'winner' column where the results were too close to call\n",
    "election.winner[too_close] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'age' and 'cabin' columns: df\n",
    "df = titanic[['age', 'cabin']].copy()\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Drop rows in df with how='any' and print the shape\n",
    "print(df.dropna(how='any').shape)\n",
    "\n",
    "# Drop rows in df with how='all' and print the shape\n",
    "print(df.dropna(how='all').shape)\n",
    "\n",
    "# Call .dropna() with thresh=1000 and axis='columns' and print the output of .info() from titanic\n",
    "print(titanic.dropna(thresh=1000, axis='columns').info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorized methods\n",
    "\n",
    ".index doesn't have .apply, it only has .map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to convert degrees Fahrenheit to degrees Celsius: to_celsius\n",
    "def to_celsius(F):\n",
    "    return 5/9*(F - 32)\n",
    "\n",
    "# Apply the function over 'Mean TemperatureF' and 'Mean Dew PointF': df_celsius\n",
    "df_celsius = weather[['Mean TemperatureF','Mean Dew PointF']].apply(to_celsius)\n",
    "\n",
    "# Reassign the columns df_celsius\n",
    "df_celsius.columns = ['Mean TemperatureC', 'Mean Dew PointC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary: red_vs_blue\n",
    "red_vs_blue = {'Obama':'blue', 'Romney':'red'}\n",
    "\n",
    "# Use the dictionary to map the 'winner' column to the new column: election['color']\n",
    "election['color'] = election['winner'].map(red_vs_blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call zscore with election['turnout'] as input: turnout_zscore\n",
    "turnout_zscore = zscore(election['turnout'])\n",
    "\n",
    "# Print the type of turnout_zscore\n",
    "print(type(turnout_zscore))\n",
    "\n",
    "# Assign turnout_zscore to a new column: election['turnout_zscore']\n",
    "election['turnout_zscore'] = turnout_zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas\n",
    "\n",
    "index: sequence of labels\n",
    "    \n",
    "series: 1D array with index\n",
    "    \n",
    "dataframes: 2D array with series as columns\n",
    "\n",
    "indexes\n",
    "\n",
    "immutable (like dictionary keys)\n",
    "\n",
    "homogenous in data type (like NumPy arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of new indexes: new_idx\n",
    "new_idx = [item.upper() for item in sales.index]\n",
    "\n",
    "# Assign the string 'MONTHS' to sales.index.name\n",
    "sales.index.name = 'MONTHS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fancy indexing is to get an inner index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index to be the columns ['state', 'month']: sales\n",
    "sales = sales.set_index(['state', 'month'])\n",
    "\n",
    "# Sort the MultiIndex: sales\n",
    "sales = sales.sort_index()\n",
    "\n",
    "# Access the data from 'NY'\n",
    "print(sales.loc['NY'])\n",
    "\n",
    "# Look up data for NY in month 1: NY_month1\n",
    "NY_month1 = sales.loc[('NY', 1), :]\n",
    "\n",
    "# Look up data for CA and TX in month 2: CA_TX_month2\n",
    "CA_TX_month2 = sales.loc[(['CA', 'TX'], 2), :]\n",
    "\n",
    "# Look up data for all states in month 2: all_month2\n",
    "all_month2 = sales.loc[(slice(None), 2), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoting\n",
    "\n",
    "reshape the data to change the columns and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors_pivot = users.pivot(index='weekday', columns='city', values='visitors')\n",
    "signups_pivot = users.pivot(index='weekday', columns='city', values='signups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack and unstack shifts to make more columns or more rows\n",
    "\n",
    "can swap the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack users by 'weekday': byweekday\n",
    "byweekday = users.unstack(level='weekday')\n",
    "\n",
    "# Stack byweekday by 'weekday' and print it\n",
    "print(byweekday.stack(level='weekday'))\n",
    "\n",
    "# Stack 'city' back into the index of bycity: newusers\n",
    "newusers = bycity.stack('city')\n",
    "\n",
    "# Swap the levels of the index of newusers: newusers\n",
    "newusers = newusers.swaplevel(0,1)\n",
    "\n",
    "# Print newusers and verify that the index is not sorted\n",
    "print(newusers)\n",
    "\n",
    "# Sort the index of newusers: newusers\n",
    "newusers = newusers.sort_index()\n",
    "\n",
    "# Print newusers and verify that the index is now sorted\n",
    "print(newusers)\n",
    "\n",
    "# Verify that the new DataFrame is equal to the original\n",
    "print(newusers.equals(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melt converts column names to aone colum but can specify which values should remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visitors = pd.melt(visitors_by_city_weekday, id_vars=['weekday'], value_name='visitors')\n",
    "\n",
    "# Melt users: skinny\n",
    "skinny = pd.melt(users, id_vars=['weekday', 'city'])\n",
    "\n",
    "# Set the new index: users_idx\n",
    "users_idx = users.set_index(['city', 'weekday'])\n",
    "\n",
    "# Obtain the key-value pairs: kv_pairs\n",
    "kv_pairs = pd.melt(users_idx, col_level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pivot doesn't always work, repeated pairs don't work\n",
    "\n",
    "this is where a pivot_table comes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_city_day = users.pivot_table(index='weekday', columns='city')\n",
    "\n",
    "# Use a pivot table to display the count of each column: count_by_weekday1\n",
    "count_by_weekday1 = users.pivot_table(index='weekday', aggfunc='count')\n",
    "\n",
    "# Replace 'aggfunc='count'' with 'aggfunc=len': count_by_weekday2\n",
    "count_by_weekday2 = users.pivot_table(index='weekday', aggfunc=len)\n",
    "\n",
    "# Add in the margins: signups_and_visitors_total \n",
    "signups_and_visitors_total = users.pivot_table(index='weekday', aggfunc=sum, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby\n",
    "\n",
    "groupby customers and sum on bread column\n",
    "\n",
    "categorical variables use less memory and speed up operations like groupby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group titanic by 'embarked' and 'pclass'\n",
    "by_mult = titanic.groupby(['embarked', 'pclass'])\n",
    "\n",
    "# Aggregate 'survived' column of by_mult by count\n",
    "count_mult = by_mult['survived'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read life_fname into a DataFrame: life\n",
    "life = pd.read_csv(life_fname, index_col='Country')\n",
    "\n",
    "# Read regions_fname into a DataFrame: regions\n",
    "regions = pd.read_csv(regions_fname, index_col='Country')\n",
    "\n",
    "# Group life by regions['region']: life_by_region\n",
    "life_by_region = life.groupby(regions['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group titanic by 'pclass': by_class\n",
    "by_class = titanic.groupby('pclass')\n",
    "\n",
    "# Select 'age' and 'fare'\n",
    "by_class_sub = by_class[['age','fare']]\n",
    "\n",
    "# Aggregate by_class_sub by 'max' and 'median': aggregated\n",
    "aggregated = by_class_sub.agg(['max', 'median'])\n",
    "\n",
    "# Print the maximum age in each class\n",
    "print(aggregated.loc[:, ('age','max')])\n",
    "\n",
    "# Print the median fare in each class\n",
    "print(aggregated.loc[:, ('fare', 'median')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame and sort the index: gapminder\n",
    "gapminder = pd.read_csv('gapminder.csv', index_col=['Year', 'region', 'Country']).sort_index()\n",
    "\n",
    "# Group gapminder by 'Year' and 'region': by_year_region\n",
    "by_year_region = gapminder.groupby(['Year', 'region'])\n",
    "\n",
    "# Define the function to compute spread: spread\n",
    "def spread(series):\n",
    "    return series.max() - series.min()\n",
    "\n",
    "# Create the dictionary: aggregator\n",
    "aggregator = {'population':'sum', 'child_mortality':'mean', 'gdp':spread}\n",
    "\n",
    "# Aggregate by_year_region using the dictionary: aggregated\n",
    "aggregated = by_year_region.agg(aggregator)\n",
    "\n",
    "# Print the last 6 entries of aggregated \n",
    "print(aggregated.tail(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object: by_day\n",
    "by_day = sales.groupby(sales.index.strftime('%a'))\n",
    "\n",
    "# Create sum: units_sum\n",
    "units_sum = by_day['Units'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby and transform data\n",
    "\n",
    "apply when too complicated for groupby and transform\n",
    "\n",
    "normalize the Gapminder data in 2010 for life expectancy and fertility by the z-score per region. Using boolean indexing, you will filter out countries that have high fertility rates and low life expectancy for their region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group gapminder_2010: standardized\n",
    "standardized = gapminder_2010.groupby('region')[['life', 'fertility']].transform(zscore)\n",
    "\n",
    "# Construct a Boolean Series to identify outliers: outliers\n",
    "outliers = (standardized['life'] < -3) | (standardized['fertility'] > 3)\n",
    "\n",
    "# Filter gapminder_2010 by the outliers: gm_outliers\n",
    "gm_outliers = gapminder_2010.loc[outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a groupby object: by_sex_class\n",
    "by_sex_class = titanic.groupby(['sex', 'pclass'])\n",
    "\n",
    "# Write a function that imputes median\n",
    "def impute_median(series):\n",
    "    return series.fillna(series.median())\n",
    "\n",
    "# Impute age and assign to titanic['age']\n",
    "titanic.age =  by_sex_class['age'].transform(impute_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group gapminder_2010 by 'region': regional\n",
    "regional = gapminder_2010.groupby('region')\n",
    "\n",
    "# Apply the disparity function on regional: reg_disp\n",
    "reg_disp = regional.apply(disparity)\n",
    "\n",
    "# Print the disparity of 'United States', 'United Kingdom', and 'China'\n",
    "print(reg_disp.loc[['United States', 'United Kingdom', 'China']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 'Units' where the sum is > 35: by_com_filt\n",
    "by_com_filt = by_company.filter(lambda g:g['Units'].sum() > 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Boolean Series: under10\n",
    "under10 = (titanic['age'] < 10).map({True:'under 10', False:'over 10'})\n",
    "\n",
    "# Group by under10 and compute the survival rate\n",
    "survived_mean_1 = titanic.groupby(under10)['survived'].mean()\n",
    "print(survived_mean_1)\n",
    "\n",
    "# Group by under10 and pclass and compute the survival rate\n",
    "survived_mean_2 = titanic.groupby([under10, 'pclass'])['survived'].mean()\n",
    "print(survived_mean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pivotting - restructure rows and columns\n",
    "\n",
    "groupby - divide into logical categories\n",
    "\n",
    ".unique()\n",
    "\n",
    ".value_counts()\n",
    "\n",
    "filtering uses boolean series to isolate parts of the data that are intriguing \n",
    "\n",
    "idxmax() where the max value is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of medals won by each country: medal_counts\n",
    "medal_counts = country_names.value_counts()\n",
    "\n",
    "# Construct the pivot table: counted\n",
    "counted = medals.pivot_table(index='NOC', values='Athlete', columns='Medal', aggfunc='count')\n",
    "\n",
    "# Create the new column: counted['totals']\n",
    "counted['totals'] = counted.sum(axis='columns')\n",
    "\n",
    "# Sort counted by the 'totals' column\n",
    "counted = counted.sort_values('totals', ascending=False)\n",
    "\n",
    "# Drop duplicate pairs: ev_gen_uniques\n",
    "ev_gen_uniques = ev_gen.drop_duplicates()\n",
    "\n",
    "# Create the Boolean Series: sus\n",
    "sus = (medals.Event_gender == 'W') & (medals.Gender == 'Men')\n",
    "\n",
    "# Create a DataFrame with the suspicious row: suspect\n",
    "suspect = medals[sus]\n",
    "\n",
    "# Extract all rows for which the 'Edition' is between 1952 & 1988: during_cold_war\n",
    "during_cold_war = (medals['Edition'] >= 1952) & (medals['Edition'] <= 1988)\n",
    "\n",
    "# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs\n",
    "is_usa_urs = medals.NOC.isin(['USA', 'URS'])\n",
    "\n",
    "# Use during_cold_war and is_usa_urs to create the DataFrame: cold_war_medals\n",
    "cold_war_medals = medals.loc[during_cold_war & is_usa_urs]\n",
    "\n",
    "# Group cold_war_medals by 'NOC'\n",
    "country_grouped = cold_war_medals.groupby('NOC')\n",
    "\n",
    "# Create Nsports\n",
    "Nsports = country_grouped['Sport'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result reported gives the number of times each of the USA or the USSR won more Olympic medals in total than the other between 1952 and 1988."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pivot table: medals_won_by_country\n",
    "medals_won_by_country = medals.pivot_table(index='Edition', columns='NOC', values='Athlete', aggfunc='count')\n",
    "\n",
    "# Slice medals_won_by_country: cold_war_usa_usr_medals\n",
    "cold_war_usa_usr_medals = medals_won_by_country.loc[1952:1988, ['USA','URS']]\n",
    "\n",
    "# Create most_medals \n",
    "most_medals = cold_war_usa_usr_medals.idxmax(axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unstack() is a reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame: usa\n",
    "usa = medals[medals['NOC'] == 'USA']\n",
    "\n",
    "# Group usa by ['Edition', 'Medal'] and aggregate over 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame: usa\n",
    "usa = medals[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by 'Edition', 'Medal', and 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\n",
    "# Create an area plot of usa_medals_by_year\n",
    "usa_medals_by_year.plot.area()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine 'Medal' as an ordered categorical\n",
    "medals.Medal = pd.Categorical(values=medals.Medal, categories=['Bronze', 'Silver', 'Gold'], ordered=True)\n",
    "\n",
    "# Create the DataFrame: usa\n",
    "usa = medals[medals.NOC == 'USA']\n",
    "\n",
    "# Group usa by 'Edition', 'Medal', and 'Athlete'\n",
    "usa_medals_by_year = usa.groupby(['Edition', 'Medal'])['Athlete'].count()\n",
    "\n",
    "# Reshape usa_medals_by_year by unstacking\n",
    "usa_medals_by_year = usa_medals_by_year.unstack(level='Medal')\n",
    "\n",
    "# Create an area plot of usa_medals_by_year\n",
    "usa_medals_by_year.plot.area()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
